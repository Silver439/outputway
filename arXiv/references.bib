@ARTICLE{Shahriari2016taking,
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
  journal={Proceedings of the IEEE}, 
  title={Taking the Human Out of the Loop: A Review of Bayesian Optimization}, 
  year={2016},
  volume={104},
  number={1},
  pages={148-175},
  keywords={Big data;Bayes methods;Linear programming;Decision making;Design of experiments;Optimization;Genomes;Statistical analysis;decision making;design of experiments;optimization;response surface methodology;statistical learning;genomic medicine;Decision making;design of experiments;optimization;response surface methodology;statistical learning},
  doi={10.1109/JPROC.2015.2494218}}

@book{Rasmussen2005Gaussian,
    author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
    title = "{Gaussian Processes for Machine Learning}",
    publisher = {The MIT Press},
    year = {2005},
    month = {11},
    abstract = "{A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.}",
    isbn = {9780262256834},
    doi = {10.7551/mitpress/3206.001.0001},
    url = {https://doi.org/10.7551/mitpress/3206.001.0001},
}

@article{Jones1998Efficient,
author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
title = {Efficient Global Optimization of Expensive Black-Box Functions},
year = {1998},
issue_date = {December 1998},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {4},
issn = {0925-5001},
url = {https://doi.org/10.1023/A:1008306431147},
doi = {10.1023/A:1008306431147},
abstract = {In many engineering optimization problems, the number of function
evaluations is severely limited by time or cost. These problems pose a
special challenge to the field of global optimization, since existing
methods often require more function evaluations than can be comfortably
afforded. One way to address this challenge is to fit response surfaces to
data collected by evaluating the objective and constraint functions at a few
points. These surfaces can then be used for visualization, tradeoff
analysis, and optimization. In this paper, we introduce the reader to a
response surface methodology that is especially good at modeling the
nonlinear, multimodal functions that often occur in engineering. We then
show how these approximating functions can be used to construct an efficient
global optimization algorithm with a credible stopping rule. The key to
using response surfaces for global optimization lies in balancing the need
to exploit the approximating surface (by sampling where it is minimized)
with the need to improve the approximation (by sampling where prediction
error may be high). Striking this balance requires solving certain auxiliary
problems which have previously been considered intractable, but we show how
these computational obstacles can be overcome.},
journal = {J. of Global Optimization},
month = {dec},
pages = {455–492},
numpages = {38},
keywords = {Bayesian global optimization, Kriging, Random function, Response surface, Stochastic process, Visualization}
}

@inproceedings{Hansen2010RealParameterBO,
  title={Real-Parameter Black-Box Optimization Benchmarking BBOB-2010 : Experimental Setup},
  author={Nikolaus Hansen and Anne Auger and Steffen Finck and Raymond Ros},
  year={2010},
  url={https://api.semanticscholar.org/CorpusID:260830254}
}

@ARTICLE{Srinivas2009Gaussian,
       author = {{Srinivas}, Niranjan and {Krause}, Andreas and {Kakade}, Sham M. and {Seeger}, Matthias},
        title = "{Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2009,
        month = dec,
          eid = {arXiv:0912.3995},
        pages = {arXiv:0912.3995},
          doi = {10.48550/arXiv.0912.3995},
archivePrefix = {arXiv},
       eprint = {0912.3995},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009arXiv0912.3995S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Hennig2012Entropy,
author = {Hennig, Philipp and Schuler, Christian J.},
title = {Entropy search for information-efficient global optimization},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation.},
journal = {J. Mach. Learn. Res.},
month = {jun},
pages = {1809–1837},
numpages = {29},
keywords = {probability, optimization, information, expectation propagation, Gaussian processes}
}

@article{Thompson1933On,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2332286},
 author = {William R. Thompson},
 journal = {Biometrika},
 number = {3/4},
 pages = {285--294},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples},
 urldate = {2024-03-11},
 volume = {25},
 year = {1933}
}

@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE},
  doi={10.1109/SOCPAR.2014.7008025}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}

@article{Huang2006Global,
author = {Huang, D. and Allen, Theodore and Notz, William and Zheng, Ning},
year = {2006},
month = {03},
pages = {441-466},
title = {Global Optimization of Stochastic BlackBox Systems via Sequential Kriging Meta-Models},
volume = {34},
journal = {Journal of Global Optimization},
doi = {10.1007/s10898-005-2454-3}
}

@article{Forrester2006Design,
author = {Forrester, Alexander I. J. and Keane, Andy J. and Bressloff, Neil W.},
title = {Design and Analysis of "Noisy" Computer Experiments},
journal = {AIAA Journal},
volume = {44},
number = {10},
pages = {2331-2339},
year = {2006},
doi = {10.2514/1.20068},

URL = { 
        https://doi.org/10.2514/1.20068
},
eprint = {   
        https://doi.org/10.2514/1.20068   
}
}

@article{Picheny2012Quantile,
author = {Picheny, Victor and Ginsbourger, David and Richet, Yann and Caplin, Grégory},
year = {2012},
month = {03},
pages = {},
title = {Quantile-Based Optimization of Noisy Computer Experiments With Tunable Precision},
volume = {55},
journal = {Technometrics},
doi = {10.1080/00401706.2012.707580}
}

@article{Picheny2013benchmark,
author = {Picheny, Victor and Wagner, Tobias and Ginsbourger, David},
year = {2013},
month = {09},
pages = {},
title = {A benchmark of kriging-based infill criteria for noisy optimization},
volume = {48},
journal = {Structural and Multidisciplinary Optimization},
doi = {10.1007/s00158-013-0919-4}
}

@ARTICLE{Bossek2020Initial,
       author = {{Bossek}, Jakob and {Doerr}, Carola and {Kerschke}, Pascal},
        title = "{Initial Design Strategies and their Effects on Sequential Model-Based Optimization}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing},
         year = 2020,
        month = mar,
          eid = {arXiv:2003.13826},
        pages = {arXiv:2003.13826},
          doi = {10.48550/arXiv.2003.13826},
archivePrefix = {arXiv},
       eprint = {2003.13826},
 primaryClass = {cs.NE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200313826B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}